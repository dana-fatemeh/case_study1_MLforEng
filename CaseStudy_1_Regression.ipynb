{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dana-fatemeh/case_study1_MLforEng/blob/main/CaseStudy_1_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1MVCm8ccp5e"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrOTWK8JIDQR"
      },
      "source": [
        "# Polymer Melt Flow Rate\n",
        "\n",
        "This case study is based upon the excellent example below:\n",
        "\n",
        "http://apmonitor.com/pds/index.php/Main/PolymerMeltFlowRate\n",
        "\n",
        "Along with its github repository:\n",
        "\n",
        "https://github.com/APMonitor/pds\n",
        "\n",
        "and license:\n",
        "\n",
        "https://github.com/APMonitor/pds/blob/main/LICENSE\n",
        "\n",
        "In particular, as described on the web page above:\n",
        "\n",
        "Polymer properties such as density, melt index, and melt flow rate must be kept within tight specifications for each grade. This case study is to analyze polymer production data to predict melt flow rate.\n",
        "\n",
        "**Background:** There are gas phase and liquid slurry reactors that create polymers (polyethylene, polypropylene, polystyrene, and others) from chemical building blocks known as monomers (C2=, C3=, C4=, iC5=, and others). A catalyst is injected with the monomers under carefully controlled temperature and pressure conditions to cause a reaction that grows the polymer chains. Hydrogen is a chain transfer agent to stop the growth of the polymer chain. If the polymer chains grow too long then the polymer is too viscous for manufacturing in films, injection molding, or other applications.\n",
        "\n",
        "This case study focuses on measurements of Melt Flow Rate (MFR) to determine the polymer viscosity based on reactor conditions. An accurate model is desirable so that the infrequent lab samples (every 2-8 hours) are supplemented with a virtual and continuous \"soft sensor\". A model that runs in real-time simulation alongside the physical reactor is called a digital twin.\n",
        "\n",
        "| Label | Data File Tag   | Description |\n",
        "|------|------|------|\n",
        "|   Time  | | Timestamp of the measurements|\n",
        "|   C3  | 513FC31103.pv| Propylene (C3=) Feed Rate (kg/hr)|\n",
        "|   H2R  | 513HC31114-5.mv| Hydrogen to C3= Ratio|\n",
        "|   Pressure  | 513PC31201.pv| Reactor Pressure (bar)|\n",
        "|   Level  | 513LC31202.pv| Reactor Bed Level (m)|\n",
        "|   C2  | 513FC31409.pv| Ethylene (C2=) Flow (kg/hr)|\n",
        "|   Cat  | 513FC31114-5.pv| Catalyst Feed Rate (kg/hr)|\n",
        "|   Temp  | 513TC31220.pv| Reactor Temperature|\n",
        "|   MFR  | MFR| Melt Flow Rate (gm/10min)|\n",
        "\n",
        "**References**\n",
        "\n",
        "Hedengren, J. D. (2021, December 16). Polymer Melt Flow Rate, Machine Learning for Engineers. Retrieved from http://apmonitor.com/pds/index.php/Main/PolymerMeltFlowRate\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iUrEej1e0wP"
      },
      "source": [
        "# Naive solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_pCHcmzUvL-"
      },
      "source": [
        "### Import Polymer MFR Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnU57p4jcp5i"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu6FpXRfV_6-"
      },
      "source": [
        "Don't worry, the data has been cached on the github page for this class so you can access it directly by url. Easy!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89hV3SYRXdhL"
      },
      "outputs": [],
      "source": [
        "url = 'https://raw.githubusercontent.com/rcpaffenroth/DS5006-Machine-Learning-for-Engineering-and-Science-Applications/main/data/polymer_reactor.txt'\n",
        "# url = 'polymer_reactor.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQmWTe06XxUf"
      },
      "source": [
        "Let's read the data through pandas.  It would be great to get to know pandas. It is an important tool for Data Science in Python.\n",
        "\n",
        "![Book cover](https://images-na.ssl-images-amazon.com/images/I/51HuYEwAl2L._SX258_BO1,204,203,200_.jpg)\n",
        "\n",
        "https://images-na.ssl-images-amazon.com/images/I/51HuYEwAl2L._SX258_BO1,204,203,200_.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMh534aZXnS1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# We can pass the url directly into pandas read_csv file\n",
        "# to read a dataframe directly\n",
        "data = pd.read_csv(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "101hNJwNX8n0"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RM_nPmV2YWUA"
      },
      "source": [
        "What does all this mean? 513FC31103.pv\t513HC31114-5.mv\t513PC31201.pv\t513LC31202.pv\t513FC31409.pv\t513FC31114-5.pv\t513TC31220.pv.   What do the column labels mean?  You probably want to map them to something more meaningful.\n",
        "\n",
        "![?](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR2W-Aq7YoFONiyUsix1x8wXnlesgqEyDShTA&usqp=CAU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZS3EuTZJYfaF"
      },
      "outputs": [],
      "source": [
        "data = data.set_axis(['Time','C3','H2R','Pressure','Level','C2','Cat','Temp','MFR'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgAbQKeYaJ-o"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8I65vTC1aaja"
      },
      "source": [
        "Nice, now I know what they are. üôÇ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OumEPLklfFQa"
      },
      "source": [
        "## Regression Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ax5fciXoeXs7"
      },
      "source": [
        "Now, we have the data and we know what they are. We learned a lot from Randy's üßô lecture and can't wait to try the machine learning algorithms. Let's start."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_kj9S9HgF63"
      },
      "source": [
        "### Divide Data\n",
        "\n",
        "What is the first step? Wow, Randy üßô  said we need to divide the training set and the test set.\n",
        "\n",
        "\"Data is divided into train and test sets to separate a fraction of the rows for evaluating classification or regression models. A typical split is 80% for training and 20% for testing, although the range depends on how much data is available and the objective of the study.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LOs6wXjiauR"
      },
      "source": [
        "The `train_test_split` is a function in `sklearn` for the specific purpose of splitting data into train and test sets.\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "train,test = train_test_split(ds, test_size=0.2, shuffle=True)\n",
        "```\n",
        "\n",
        "There are options such as `shuffle=True` to randomize the selection in each set.\n",
        "\n",
        "One of those arguments is `random_state`; this argument, when set to a value, determines which rows go into the train and test splits. Make a mental note of the value we set, we may see it again later in the notebook! üßô"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZN_UPCSgGxp"
      },
      "outputs": [],
      "source": [
        "# !pip install scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# note how we set the random_state\n",
        "train,test = train_test_split(data, test_size=0.2, shuffle=True, random_state=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGf32KQ7pISp"
      },
      "source": [
        "Can't wait any longer to run the model?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLi7Pclyqq_t"
      },
      "source": [
        "#### Run model\n",
        "Let's try linear regression using sklearn!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MV7NxOzqODv"
      },
      "outputs": [],
      "source": [
        "from sklearn import linear_model\n",
        "method = linear_model.LinearRegression()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBy06A-yrfrg"
      },
      "source": [
        "What columns are used as features? What columns are used as labels?\n",
        "Don't know? Then let's use them all first, except for time, since time is not important in this case.\n",
        "\n",
        "**FeaturesÔºö**`C3, H2R, Pressure, Level, C2, Cat, Temp`\n",
        "\n",
        "**labelÔºö** `MFR`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pu7uX7NcqOO3"
      },
      "outputs": [],
      "source": [
        "# We call method.fit() to learn the parameters of our network\n",
        "# to the specific data\n",
        "\n",
        "model = method.fit(\n",
        "   X = train[['C3', 'H2R', 'Pressure', 'Level', 'C2', 'Cat', 'Temp']].to_numpy(),\n",
        "   y = train['MFR'].to_numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bv4pdbceuLme"
      },
      "source": [
        "ValueError: Input contains NaN????\n",
        "\n",
        "![?](https://www.memecreator.org/static/images/memes/4724747.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmFMLOVcuuod"
      },
      "source": [
        "Wow! We forgot to check if there are any null values in the dataset. üòû"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU_SKRrLvRfy"
      },
      "source": [
        "\n",
        "`data.isnull()`  is used to check if there is null value in the dataset. We can sum them by using sum().\n",
        "\n",
        "`data.info()` and `data.describe()` are usually used to check the properties and basic statistics of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUNuUEAgw7wU"
      },
      "outputs": [],
      "source": [
        "# Let's check how many total missing values exist in our training set\n",
        "train.isnull().sum().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cl1cvAsExfpl"
      },
      "outputs": [],
      "source": [
        "# Let's do the same for the test set we made\n",
        "test.isnull().sum().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jy95QAhEuYs9"
      },
      "outputs": [],
      "source": [
        "# use train.info() to get each column's number of non-missing values (Non-Null Count)\n",
        "# Note: train.info() is also useful to see what kind a data type (Dtype) pandas\n",
        "  # has encoded the columns. Sometimes pandas might mistakenly think a column of numbers\n",
        "  # is actually a column of strings, so always check beforehand!\n",
        "train.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ucVJou0uYvq"
      },
      "outputs": [],
      "source": [
        "train.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaXkhW_iuWsY"
      },
      "source": [
        "Yes, null values do exist in the dataset, let's delete them first.\n",
        "\n",
        "`data.dropna()` is used to remove the rows that contain at least one null value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nt394ksctdGz"
      },
      "outputs": [],
      "source": [
        "# assign new variables to versions of the data with no\n",
        "# missing values\n",
        "train_dropna = train.dropna()\n",
        "test_dropna = test.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXj-NOH9wb3e"
      },
      "outputs": [],
      "source": [
        "train_dropna.isnull().sum().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfXpbEDrxmb5"
      },
      "outputs": [],
      "source": [
        "test_dropna.isnull().sum().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joB7Qpf8wNHQ"
      },
      "source": [
        "Now, let's try to run the model again!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGgpeWVTwMNR"
      },
      "outputs": [],
      "source": [
        "model = method.fit(\n",
        "    X=train_dropna[['C3', 'H2R', 'Pressure', 'Level', 'C2', 'Cat', 'Temp']].to_numpy(),\n",
        "    y=train_dropna['MFR'].to_numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4hZu7Naxsyy"
      },
      "source": [
        "Great!!! The model was successfully fitÔºÅ üôÇ Now, we can evaluate the performance on test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aD0a35syGnV"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "r2 = method.score(test_dropna[['C3', 'H2R', 'Pressure', 'Level', 'C2', 'Cat', 'Temp']].to_numpy(),test_dropna['MFR'].to_numpy())\n",
        "mse = mean_squared_error(method.predict(test_dropna[['C3', 'H2R', 'Pressure', 'Level', 'C2', 'Cat', 'Temp']].to_numpy()), test_dropna['MFR'].to_numpy())\n",
        "\n",
        "print('R^2: ' + str(r2))\n",
        "print('mse: ' + str(mse))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ez8IOR2KzsNk"
      },
      "source": [
        "Ok, we already see things are interesting.  There are two different metrics (R^2 and mse)!   Hmmm...."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsoQbvLK5ZfO"
      },
      "source": [
        "## **Question 1** **(10 points)**\n",
        "\n",
        "**Part 1 (5 points)**\n",
        "\n",
        "What do R^2 and mse mean? Which is better for describing the performance of your algorithm?  Why?\n",
        "\n",
        "__Answer__:\n",
        "- R^2: test push\n",
        "- MSE (mean squared error):\n",
        "\n",
        "**Part 2 (5 points)**  \n",
        "\n",
        "Try different variables to fit the model and test its performance.   What do you notice?  Are some worse and some better?\n",
        "\n",
        "Hint:\n",
        "`model = method.fit(train_dropna[['Your selected variable 1', 'Your selected variable 2']].to_numpy(),train_dropna['MFR'].to_numpy())`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuQjxaTscp53"
      },
      "source": [
        "## **Answer 1**\n",
        "\n",
        "Part 1: ùëÖ^2 tells us how much of the variation in the real target values (MFR) is explained by the model. An ùëÖ^2 of 1 means the model explains everything perfectly, 0 means it's no better than just predicting the average, and it can even be negative if the model is worse than that baseline.\n",
        "MSE (mean squared error) is the average of (prediction ‚àí true)^2. It directly measures how far predictions are from the real values, and smaller is better. Because it squares the errors, it also punishes large mistakes more. In practice, both are useful, but if we want to understand how wrong our predictions are in a concrete way, MSE is usually more informative because it directly reflects prediction error size. ùëÖ^2 is helpful as a quick explanation summary, but it doesn't tell you the real error magnitude.\n",
        "\n",
        "\n",
        "Part 2: When we try different sets of variables, the performance changes. Some combinations give a higher ùëÖ^2 and lower MSE (better), and some give a lower ùëÖ^2 and higher MSE (worse). In general, variables that have a real relationship with MFR improve the results, while weak or noisy variables don't help much and can sometimes make test performance worse. Also, adding more variables does not always improve the model. If the new variables don't add new information or they are redundant, the improvement can be small.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42gRzCGT6ckW"
      },
      "outputs": [],
      "source": [
        "# Your code here\n",
        "# Your code here\n",
        "# Your code here\n",
        "# Your code here\n",
        "# Your code here\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "train = train.dropna(subset=['C3','H2R','Pressure','Temp','Cat','Level','MFR'])\n",
        "\n",
        "def eval_features(feature_list, target='MFR'):\n",
        "    X_train = train[feature_list].to_numpy()\n",
        "    y_train = train[target].to_numpy()\n",
        "\n",
        "    X_test = test_dropna[feature_list].to_numpy()\n",
        "    y_test = test_dropna[target].to_numpy()\n",
        "\n",
        "    model = LinearRegression().fit(X_train, y_train)\n",
        "\n",
        "    r2 = model.score(X_test, y_test)\n",
        "    mse = mean_squared_error(y_test, model.predict(X_test))\n",
        "\n",
        "    print(f\"Features: {feature_list}\")\n",
        "    print(f\"R^2: {r2:.4f}\")\n",
        "    print(f\"MSE: {mse:.4f}\\n\")\n",
        "\n",
        "eval_features(['C3', 'H2R', 'Pressure', 'Temp'])\n",
        "eval_features(['Cat', 'Level'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8trV7ae7DAb"
      },
      "source": [
        "# Solution with Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPXeWSgR7VnT"
      },
      "source": [
        "How did you choose the variables? Why use these variables instead of others?\n",
        "\n",
        "![?](https://media.makeameme.org/created/its-a-feature-5b167b.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBNbdc7YDrD2"
      },
      "source": [
        "How about we create a heatmap to show the correlation between the variables?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PFKSmzg6cnO"
      },
      "outputs": [],
      "source": [
        "# import plotting libraries:\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import plotly.express as px"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73yQcSu6NNcy"
      },
      "source": [
        "Still remember? Removing null values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEMEwC0mNHhk"
      },
      "outputs": [],
      "source": [
        "data = data.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFJIfdMy3h9v"
      },
      "outputs": [],
      "source": [
        "# Drop the Time column in data, added try-pass block to be able to re-run without throwing error that \"Time\" column is already gone\n",
        "try:\n",
        "    data = data.drop(['Time'], axis=1)\n",
        "except:\n",
        "    pass\n",
        "data # cleaned dataset (no NaN's, no Time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNBYNv75EC5Y"
      },
      "source": [
        "we can create a heatmap using `seaborn.heatmap()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EL7yxOJ_6cqF"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "\n",
        "cor = data.corr()\n",
        "sns.heatmap(cor, annot=True,cmap=plt.cm.Reds)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTzGz6IHEWZ7"
      },
      "source": [
        "## **Question 2** **(20 points)**\n",
        "\n",
        "**Part 1 (5 points)**\n",
        "What does correlation mean?\n",
        "\n",
        "__Answer__: the Pandas ```.corr()``` function, without any parameters will - by default - compute the _Pearson Correlation Coefficient (PCC)_. The PCC measures the linear correlation between data normalized to the range of -1 to 1, where 1 is a result of a perfect positive _linear_ correlation, -1 a perfect negative _linear_ correlation and where 0 means no _linear_ correlation. So here, \"correlation\" refers to the linear fit of the individual datastreams with eachother - like how \"Pressure\" linearly relates to \"Temp\" in ```data```.\n",
        "\n",
        "**Part 2 (10 points)**\n",
        "What are your observations? Which variables are strongly correlated with `MFR` and which variables are weakly correlated with `MFR`?\n",
        "\n",
        "__Answer__: the variables \"H2R\", \"Pressure\", and trivially \"MFR\" all have the _strongest_ (positive) correlations with `MFR`, though I would clasify \"strong\" as a lot closer to -1 or 1. \"Level\" has nearly no correlation, and all the rest are weakly correlated with `MFR`.\n",
        "\n",
        "\n",
        "**Part 3 (5 points)**\n",
        "Is a variable with a large *negative* correlation useful or not?\n",
        "\n",
        "__Answer__: Of course! A correlation is a correlation, does not matter which direction their trend goes, just that the trend is there.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvKE7AavFiJn"
      },
      "source": [
        "### Pair Plot\n",
        "\n",
        "A pair plot shows the correlation between variables.\n",
        "\n",
        "```python\n",
        "sns.pairplot(data)\n",
        "```\n",
        "\n",
        "It has bar distributions on the diagonal and scatter plots on the off-diagonal. A pair plot also shows a different color (`hue`) by category `w`. Pair plots show correlations between pairs of variables that may be related and gives a good indication of features (explanatory inputs) that are used for classification or regression. Generate your own pair plot of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FG0mlIIF6cwF"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(data) # This will run for a little bit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JSvHB5EIhaC"
      },
      "source": [
        "### Joint Plot\n",
        "\n",
        "Want to change the plot style? No problem! üëå\n",
        "\n",
        "A joint plot shows two variables, with the univariate and joint distributions.\n",
        "\n",
        "```python\n",
        "sns.jointplot(x='H2R',y='MFR',data=data,kind=\"kde\")\n",
        "```\n",
        "\n",
        "Generate your own joint plot with the data. Try `kind='reg'`, `'kde'`, and `'hex'` to see different joint plot styles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UT9Zu_J1Ij5v"
      },
      "outputs": [],
      "source": [
        "sns.jointplot(x='H2R',y='MFR',data=data,kind=\"kde\") # Professor's default plot\n",
        "sns.jointplot(x='H2R',y='MFR',data=data,kind=\"hex\") # plot Ed made to test another style"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyUIFPnpKy0f"
      },
      "source": [
        "### Data Analysis with `ydata-profiling` (formerly known as pandas-profiling)\n",
        "\n",
        "ydata - Profiling is a data analysis tool for a more in-depth summary of the data than the `descibe()` function. [Install the package](https://docs.profiling.ydata.ai/latest/getting-started/installation/) with:\n",
        "\n",
        "```python\n",
        "pip install --user ydata-profiling[notebook]\n",
        "jupyter nbextension enable --py widgetsnbextension\n",
        "```\n",
        "\n",
        "<!-- **This idea is important!** You will need to install more packages than Colab provides, and this is an example of doing that.\n",
        "\n",
        "NOTE:  You need to restart the Kernel before proceeding. The install only needs to run once.  We will talk about this in class. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CHb8rfd_ECQ"
      },
      "outputs": [],
      "source": [
        "# first need to uninstall a conflicting version of a package,\n",
        "# then install ydata-profiling\n",
        "#!pip uninstall typing-extensions -y\n",
        "#!pip install -U ydata-profiling[notebook] typing-extensions==4.6.0\n",
        "\n",
        "# this line allows to use cool interactive widgets with a notebook\n",
        "# which we will see in action in a few code cells below\n",
        "#!jupyter nbextension enable --py widgetsnbextension\n",
        "!pip install ydata-profiling[notebook]\n",
        "!jupyter nbextension enable --py widgetsnbextension\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WV9TDP-jwWGC"
      },
      "outputs": [],
      "source": [
        "from ydata_profiling import ProfileReport"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HdUB8c9MDVG"
      },
      "outputs": [],
      "source": [
        "# here we generate the big report\n",
        "profile = ProfileReport(data, minimal=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7tt0sKlX911"
      },
      "source": [
        "ProfileReport takes a little time to render the results, please be patient. Next, we can view the report interactively with `profile.to_widgets()`\n",
        "\n",
        "IMPORTANT NOTE: If you run the following cell and nothing appears, or you get some sort of warning message, try re-running the `profile = ProfileReport(data, minimal=False)` cell again, and then re-run `profile_to_widgets()` cell. The ProfileReport can be a little finicky with Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peoTzWXCMDX7"
      },
      "outputs": [],
      "source": [
        "# This will make nice interface in the notebook\n",
        "profile.to_notebook_iframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAYsTsf2OKQy"
      },
      "source": [
        "## **Question 3** **(10 points)**\n",
        "\n",
        "Notice that the output of the previous cell is a collection of interactive tabs that contain useful information about our data. Expore the tabs to get familiar with why type of information is present.\n",
        "\n",
        "Next, find the two most \"relevant\" variables to `MFR` and fit them with linear regression. Cut and paste their \"Interaction\" plots here.  Why do you think they are relevant?\n",
        "\n",
        "Compare the results (r^2 and mse) using these variables with your previous experiments.\n",
        "\n",
        "__Answer__: PROVIDE YOUR EXPLANATIONS HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-xnJ7Bqcp57"
      },
      "source": [
        "## **Answer 3**\n",
        "\n",
        "From the interaction plots (ex: heatmap for correlation between features), Pressure and H2R look like the two variables that matter the most for predicting MFR (H2R_cor = 0.43 and Pressure_cor = 0.44). Their plots show a clearer pattern with MFR compared to the other inputs, so they don‚Äôt look like random scatter.\n",
        "When we fit a linear regression using only Pressure and H2R, the performance is okay, but it is not as good as using all the variables. With only those two features, the test ùëÖ^2 is about 0.328 and the MSE is about 17.505. With all seven features, the test ùëÖ^2 improves to about 0.426 and the MSE drops to about 14.936 (please see the code bellow:). This tells us Pressure and H2R are important, but the other variables still add useful information that helps the model make better predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzwpe-XCPJ3c"
      },
      "outputs": [],
      "source": [
        "# Your code here\n",
        "# Your code here\n",
        "# Your code here\n",
        "# Your code here\n",
        "# Your code here\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "def eval_features(train_df, test_df, features, target='MFR'):\n",
        "    X_train = train_df[features].to_numpy()\n",
        "    y_train = train_df[target].to_numpy()\n",
        "\n",
        "    X_test = test_df[features].to_numpy()\n",
        "    y_test = test_df[target].to_numpy()\n",
        "\n",
        "    model = LinearRegression().fit(X_train, y_train)\n",
        "    r2 = model.score(X_test, y_test)\n",
        "    mse = mean_squared_error(y_test, model.predict(X_test))\n",
        "\n",
        "    print(\"Features:\", features)\n",
        "    print(\"R^2:\", r2)\n",
        "    print(\"MSE:\", mse)\n",
        "    print()\n",
        "\n",
        "\n",
        "eval_features(train, test_dropna, ['Pressure', 'H2R'])\n",
        "\n",
        "eval_features(train, test_dropna, ['C3', 'H2R', 'Pressure', 'Level', 'C2', 'Cat', 'Temp'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHTloVtOQrkH"
      },
      "source": [
        "Tired of manually selecting variables? We can use the model to select best features (variables) automatically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeTtYIVVSxNN"
      },
      "source": [
        "### Select Best Features\n",
        "\n",
        "We can rank the features to determine the best set that predicts `MFR`.\n",
        "\n",
        "```python\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "best = SelectKBest(score_func=f_regression, k='all')\n",
        "fit = best.fit(x,z)\n",
        "plt.bar(x=x.columns,height=fit.scores_)\n",
        "```\n",
        "\n",
        "There is additional information on [Select K Best Features](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQyVCiNsTEBq"
      },
      "outputs": [],
      "source": [
        "x=data[['C3', 'H2R', 'Pressure', 'Level', 'C2', 'Cat', 'Temp']]\n",
        "z=data[['MFR']]\n",
        "\n",
        "# These are various feature selection algorithms available\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "# import matplotlib.pylab as plt # ED: commented this out since we already have plt from an earlier cell\n",
        "\n",
        "# First, instantiate the selection algorithm with the scoring criteria\n",
        "# here is the documentation on the scoring criteria we selected:\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html#sklearn.feature_selection.f_regression\n",
        "best = SelectKBest(score_func=f_regression, k='all')\n",
        "\n",
        "# fit the selection algorithm on our data and return\n",
        "# the f-statistic for each variable; the higher the score,\n",
        "# the more \"significant\" the variable is\n",
        "fit = best.fit(x,z)\n",
        "plt.bar(x=x.columns,height=fit.scores_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlHLQDsgT0J5"
      },
      "source": [
        "## **Question 4** **(10 points)**\n",
        "\n",
        "Based on the above results, select the variables you think are related to `MFR` and fit them with linear regression. Compare the results with the previous experiments.  What do observe?\n",
        "\n",
        "__Answer__: PROVIDE YOUR OBSERVATIONS HERE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eqlYsaXVChB"
      },
      "outputs": [],
      "source": [
        "# ED's code for linear fit of \"Pressure\" and \"H2R\" with \"MFR\":\n",
        "plot1 = sns.jointplot(x='MFR',y='H2R',data=data,kind=\"reg\",color=\"green\")\n",
        "plot1.figure.suptitle(\"MFR linear regression relationship with H2R\", y=1.02)\n",
        "\n",
        "plot2 = sns.jointplot(x='MFR',y='Pressure',data=data,kind=\"reg\")\n",
        "plot2.figure.suptitle(\"MFR linear regression relationship with Pressure\", y=1.02)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGBrPNA1VbDd"
      },
      "source": [
        "# Solution with Feature Engineering + Data processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM_iAdD6W4qi"
      },
      "source": [
        "The result of the model is not good enough? Still want to improve the performance further?\n",
        "\n",
        "Randy üßô has a few questions for you. Are there any outliers in the data? Do all variables satisfy the assumptions of the linear model?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hrltaU1ZCOW"
      },
      "source": [
        "There are several graphical techniques to help detect outliers. A box or histogram plot shows outlying points. Make one now and examine it for outliers in each variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVzSJJgMTuu3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.figure(figsize=(12,8))\n",
        "\n",
        "# iterate over all of the columns\n",
        "for i,c in enumerate(data.columns):\n",
        "    if c != \"MFR\":\n",
        "        plt.subplot(2,4,i+1)\n",
        "        plt.title(c)\n",
        "        plt.boxplot(data[c])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cV39e6OfZwXP"
      },
      "source": [
        "Remove outliers by removing select rows such as with:\n",
        "\n",
        "```python\n",
        "data = data[data['H2R']<0.7]\n",
        "data = data[data['H2R']>0.01]\n",
        "```\n",
        "\n",
        "to keep only values of `H2R` (Hydrogen to Monomer ratio) that are between 0.01 and 0.7."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgxS-3laTuxY"
      },
      "outputs": [],
      "source": [
        "data = data[data['H2R']<0.7]\n",
        "data = data[data['H2R']>0.01]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OspcdSWAZ4e4"
      },
      "source": [
        "Show the boxplot again to verify that the data set does not have the outliers you removed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ULzYkysZ8KM"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.figure(figsize=(12,8))\n",
        "\n",
        "# iterate over all of the columns\n",
        "for i,c in enumerate(data.columns):\n",
        "    if c != \"MFR\":\n",
        "        plt.subplot(2,4,i+1)\n",
        "        plt.title(c)\n",
        "        plt.boxplot(data[c])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdIJxQBJaD55"
      },
      "source": [
        "## **Question 5** **(10 points)**\n",
        "\n",
        "Are there any other outliers in other variables? If so, please remove them.  How did you go about defining outliers?\n",
        "\n",
        "__Answer__: PROVIDE YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5PZYW8Dcp5-"
      },
      "source": [
        "## **Answer 5**\n",
        "\n",
        "We checked the rest of the variables for outliers by looking for values that were unusually far from the typical range. We defined outliers using the IQR (Interquartile Range) rule: for each variable, We found Q1 and Q3, computed the IQR = Q3 ‚àí Q1, and treated anything below Q1 ‚àí 1.5√óIQR or above Q3 + 1.5√óIQR as an outlier. If a row was an outlier in any variable, We removed that row. We used the training set to compute the outlier limits and then applied the same limits to both the training and test sets so we don‚Äôt accidentally learn information from the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TSyMo8KZ0_W"
      },
      "outputs": [],
      "source": [
        "# Provide any code for Question 5 here\n",
        "import numpy as np\n",
        "\n",
        "FEATURES = ['C3', 'H2R', 'Pressure', 'Level', 'C2', 'Cat', 'Temp']\n",
        "TARGET = 'MFR'\n",
        "\n",
        "\n",
        "def iqr_bounds(df, cols, k=1.5):\n",
        "    bounds = {}\n",
        "    for col in cols:\n",
        "        q1 = df[col].quantile(0.25)\n",
        "        q3 = df[col].quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "        low = q1 - k * iqr\n",
        "        high = q3 + k * iqr\n",
        "        bounds[col] = (low, high)\n",
        "    return bounds\n",
        "\n",
        "def remove_outliers_with_bounds(df, bounds):\n",
        "    mask = np.ones(len(df), dtype=bool)\n",
        "    for col, (low, high) in bounds.items():\n",
        "        mask &= df[col].between(low, high)\n",
        "    return df[mask].copy()\n",
        "\n",
        "train_clean = train.dropna(subset=FEATURES + [TARGET]).copy()\n",
        "test_clean  = test_dropna.dropna(subset=FEATURES + [TARGET]).copy()\n",
        "\n",
        "bounds = iqr_bounds(train_clean, FEATURES + [TARGET], k=1.5)\n",
        "\n",
        "train_no_out = remove_outliers_with_bounds(train_clean, bounds)\n",
        "test_no_out  = remove_outliers_with_bounds(test_clean, bounds)\n",
        "\n",
        "print(\"Train size before:\", len(train_clean), \"after:\", len(train_no_out))\n",
        "print(\"Test size before:\",  len(test_clean),  \"after:\", len(test_no_out))\n",
        "\n",
        "# removed_counts = {}\n",
        "# for col, (low, high) in bounds.items():\n",
        "#     removed_counts[col] = int((~train_clean[col].between(low, high)).sum())\n",
        "\n",
        "# print(\"rows flagged as outliers per column (train):\")\n",
        "# for k, v in removed_counts.items():\n",
        "#     print(f\"{k}: {v}\")\n",
        "\n",
        "train = train_no_out\n",
        "test_dropna = test_no_out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI7-i5cbMp6x"
      },
      "source": [
        "# Important question!\n",
        "\n",
        "Who controls the features that you process?  Are you stuck with the features provided by the original experiment?\n",
        "\n",
        "**NO!!!!**\n",
        "\n",
        "You have the power!  For example, any invertible transformation of the data can lead to new features or even new targets!   Actually any transformation whatsoever can lead to new features and targets.  Invertible just means that you can always recover your original data :-)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxaDNegjeWLT"
      },
      "source": [
        "For example, we can apply log transformation to `MFR` and try and predict that instead.  Since the log is invertible, we can just predict that, get an prediction, and then apply the inverse of the log (the exponential map).\n",
        "\n",
        "Create a new column for the natural log of (`MFR`) as `lnMFR`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2-txTgbZ1EW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "data['lnMFR'] = np.log(data['MFR'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF4M8PlCfjVG"
      },
      "source": [
        "Check the results after log transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNZmfjwJfTnr"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcrgAKMSf85s"
      },
      "source": [
        "## **Question 6** (10 points)\n",
        "\n",
        "Pick one or two different functions and apply the to `MFR` and see if a transformed `MFR` is easier to predict.  Does transforming `MFR` change what inputs give good predictions?\n",
        "\n",
        "__Answer__: EXPLAIN YOUR OBSERVATIONS HERE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duf5enMOgMpF"
      },
      "outputs": [],
      "source": [
        "# Your code here\n",
        "# Your code here\n",
        "# Your code here\n",
        "# Your code here\n",
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-1NPfmDOPpv"
      },
      "source": [
        "## **Question 7** (10 points)\n",
        "\n",
        "Just like using transformations on the target (`MFR` in this case), you can use transformations on the inputs. Can you predict a transformed `MFR` from a set of transformed inputs?\n",
        "\n",
        "Ok, things start to explode now with so many combinations. Just try a few :-)\n",
        "\n",
        "__Answer__: EXPLAIN YOUR OBSERVATIONS HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc4RGBAvcp6A"
      },
      "source": [
        "## **Answer 7**\n",
        "\n",
        "We tried predicting a transformed version of MFR using transformed inputs, and it worked better than using only the original variables in a plain linear model. We used a log transform on the target with log1p(MFR) and tested a few transformed inputs such as log1p(Pressure), log1p(H2R), squared terms like Pressure^2, and interaction terms like Pressure*H2R. The model usually performed better when the transformations made the relationships more linear and reduced the effect of extreme values. The best results came from a small mix of log and interaction features, not from adding too many random transforms. When we added too many transformed features, the model became less stable and sometimes test performance dropped, which suggests mild overfitting. So our main observation is that transformations can help a lot, but only when they are chosen based on data behavior and not added blindly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1ikhRUHOQL7"
      },
      "outputs": [],
      "source": [
        "# Your code here\n",
        "# Your code here\n",
        "# Your code here\n",
        "# Your code here\n",
        "# Your code here\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "base = ['C3', 'H2R', 'Pressure', 'Level', 'C2', 'Cat', 'Temp']\n",
        "target = 'MFR'\n",
        "\n",
        "train_q7 = train.dropna(subset=base + [target]).copy()\n",
        "test_q7  = test_dropna.dropna(subset=base + [target]).copy()\n",
        "\n",
        "def add_transforms(df):\n",
        "    out = df.copy()\n",
        "    for c in ['Pressure', 'H2R', 'Temp', 'C3', 'C2', 'Level']:\n",
        "        out[f'log1p_{c}'] = np.log1p(np.clip(out[c], a_min=0, a_max=None))\n",
        "    out['Pressure_sq'] = out['Pressure']**2\n",
        "    out['H2R_sq'] = out['H2R']**2\n",
        "    out['Pressure_H2R'] = out['Pressure'] * out['H2R']\n",
        "    out['Temp_Pressure'] = out['Temp'] * out['Pressure']\n",
        "    return out\n",
        "\n",
        "train_t = add_transforms(train_q7)\n",
        "test_t  = add_transforms(test_q7)\n",
        "\n",
        "experiments = {\n",
        "    \"baseline_raw\": base,\n",
        "    \"log_inputs_only\": ['log1p_Pressure','log1p_H2R','log1p_Temp','log1p_C3','log1p_C2','log1p_Level','Cat'],\n",
        "    \"log_plus_interactions\": ['log1p_Pressure','log1p_H2R','log1p_Temp','Pressure_H2R','Temp_Pressure','Cat'],\n",
        "    \"poly_plus_interactions\": ['Pressure','H2R','Temp','Pressure_sq','H2R_sq','Pressure_H2R','Cat']\n",
        "}\n",
        "\n",
        "def eval_model(Xtr, ytr, Xte, yte):\n",
        "    m = LinearRegression().fit(Xtr, ytr)\n",
        "    pred = m.predict(Xte)\n",
        "    return m.score(Xte, yte), mean_squared_error(yte, pred)\n",
        "\n",
        "print(\"Predicting original MFR: \")\n",
        "for name, feats in experiments.items():\n",
        "    r2, mse = eval_model(train_t[feats], train_t[target], test_t[feats], test_t[target])\n",
        "    print(f\"{name}  R^2={r2}  MSE={mse}\")\n",
        "\n",
        "print(\"Predicting transformed target log1p(MFR): \")\n",
        "ytr_log = np.log1p(np.clip(train_t[target], a_min=0, a_max=None))\n",
        "yte_log = np.log1p(np.clip(test_t[target], a_min=0, a_max=None))\n",
        "for name, feats in experiments.items():\n",
        "    r2, mse = eval_model(train_t[feats], ytr_log, test_t[feats], yte_log)\n",
        "    print(f\"{name}  R^2={r2}  MSE={mse}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctXE_2KNiVHH"
      },
      "source": [
        "### One important transformation:  Scaling the data\n",
        "\n",
        "Scale data with the `StandardScalar` from `scikit-learn`. This has the effect of making each column have zero mean and have unit variance.\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "s = StandardScaler()\n",
        "ds = s.fit_transform(data)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgKHvJIYifwx"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "# like most methods in sklearn, we first instantiate the algorithm\n",
        "s = StandardScaler()\n",
        "\n",
        "# then can use a shortcut function to fit and transform the data of interest\n",
        "ds = s.fit_transform(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do66METZiirY"
      },
      "source": [
        "The value `ds` is returned as a `numpy` array so we need to convert it back to a `pandas` `DataFrame`.\n",
        "\n",
        "```python\n",
        "ds = pd.DataFrame(ds,columns=data.columns)\n",
        "```\n",
        "\n",
        "Re-use the column names from `data`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ouvPOktilDB"
      },
      "outputs": [],
      "source": [
        "ds = pd.DataFrame(ds,columns=data.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcPOIGBliwLF"
      },
      "source": [
        "Now, we can check the results before/after scaling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yvGhn82ipdD"
      },
      "outputs": [],
      "source": [
        "data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gIp4STcimKl"
      },
      "outputs": [],
      "source": [
        "ds.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTbvYh5yIfiF"
      },
      "source": [
        "Note how the mean of each column is practically equal to 0, and the standard deviation is practically equal to 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5s4ocBbjWRF"
      },
      "source": [
        "\n",
        "\n",
        "Next, we resplit the processed dataset into training and test set.\n",
        "\n",
        "**Set a same random_state=1 to ensure the splitting of data in this step is the same as the splitting of data in the step earlier in the notebook.**\n",
        "\n",
        "NOTE:  We will come back to this important point later üßô"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7sMDzkLimOo"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_processed,test_processed = train_test_split(ds, test_size=0.2, shuffle=True, random_state=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFnoPRN1j0z8"
      },
      "source": [
        "## **Question 8** (10 points)\n",
        "\n",
        "Using the trained data to train the linear regression model, and report the results. Do models perform better? Why?\n",
        "\n",
        "__Answer__: PROVIDE YOUR OBSERVATIONS OF RESULTS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itaIQ9vrkzkW"
      },
      "source": [
        "Hint:\n",
        "```python\n",
        "method = linear_model.LinearRegression()\n",
        "\n",
        "model = method.fit(train_processed[['variable 1','variable 2']].to_numpy(),train_processed['MFR'].to_numpy())\n",
        "\n",
        "r2 = method.score(test_processed[['variable 1','variable 2']].to_numpy(),test_processed['MFR'].to_numpy())\n",
        "\n",
        "mse = mean_squared_error(method.predict(test_processed[['variable 1','variable 2']].to_numpy()), test_processed['MFR'].to_numpy())\n",
        "\n",
        "print('R^2: ' + str(r2))\n",
        "\n",
        "print('mse: ' + str(mse))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCnkcTJzHG-b"
      },
      "outputs": [],
      "source": [
        "# Your code here\n",
        "# Your code here\n",
        "# Your code here\n",
        "# Your code here\n",
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dImz2CMl0Hl"
      },
      "source": [
        "# More advanced solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvAfbPlvmdSK"
      },
      "source": [
        "Want to learn more? Let's look at a more advanced solution.\n",
        "\n",
        "![?](https://i.imgflip.com/2l2aus.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waQpnbaZnX6V"
      },
      "source": [
        "Let's try a count plot to show the number of binned samples of `lnMFR`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iP8v0c-jnkb4"
      },
      "outputs": [],
      "source": [
        "sns.countplot(x=ds['lnMFR'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-valxpGwPabf"
      },
      "outputs": [],
      "source": [
        "plt.plot(ds['C2'], ds['MFR'],'b.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCMlBDCinuTe"
      },
      "source": [
        "Note the interesting distribution of the data- it has a multimodal Gaussian distribution, in this case with two peaks.\n",
        "\n",
        "Why not split the data into two parts, one for each Gaussian, then perform regression on each part separately? Will this regression perform better than a single regression algorithm for the full dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19kQaPUOoKaE"
      },
      "source": [
        "First we split the data into two parts, then make train/test splits of each part with"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUcfhU26oONJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "lower_data = ds[ds['lnMFR']<0]\n",
        "upper_data = ds[ds['lnMFR']>0]\n",
        "lower_train,lower_test = train_test_split(lower_data, test_size=0.2, shuffle=True,random_state=1)\n",
        "upper_train,upper_test = train_test_split(upper_data, test_size=0.2, shuffle=True,random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84HIiyZEPvE8"
      },
      "outputs": [],
      "source": [
        "plt.plot(lower_data['Temp'], lower_data['MFR'],'b.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNJIrAEkP8bl"
      },
      "outputs": [],
      "source": [
        "plt.plot(upper_data['Temp'], upper_data['MFR'],'b.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_0AsyI9ohCQ"
      },
      "source": [
        "Run the following code to test linear regression on the full data set as well as the subsets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3DvTsYAoORT"
      },
      "outputs": [],
      "source": [
        "def fits(data):\n",
        "    X = ['Temp']\n",
        "    y = ['MFR']\n",
        "    # change this if you applied log transformation to your selected variables.\n",
        "    method = linear_model.LinearRegression()\n",
        "\n",
        "    model = method.fit(data[X].to_numpy(), data[y].to_numpy())\n",
        "\n",
        "    MFR_pred = method.predict(data[X].to_numpy())\n",
        "    r2_model = method.score(data[X].to_numpy(),data['lnMFR'].to_numpy())\n",
        "    mse_model = mean_squared_error(method.predict(data[X].to_numpy()), data['lnMFR'].to_numpy())\n",
        "\n",
        "    print('R^2: ' + str(r2_model))\n",
        "    print('mse: ' + str(mse_model))\n",
        "\n",
        "    plt.plot(data[X[0]], data[y],'b.', label=\"Real Data\")\n",
        "    plt.plot(data[X[0]], MFR_pred,'r.', label=\"Predictions\")\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5U2UXICRfBR"
      },
      "outputs": [],
      "source": [
        "fits(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPcD49P2Re4t"
      },
      "outputs": [],
      "source": [
        "fits(lower_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wG2nl2w6oOTs"
      },
      "outputs": [],
      "source": [
        "fits(upper_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Th2yyertqvJY"
      },
      "source": [
        "## **Question 9** (10 points)\n",
        "\n",
        "Explain what is going on here?  Was it ever a good idea to use linear regression on the whole dataset!?  In particular, what does it mean that the R^2 and mse changed so much?\n",
        "\n",
        "__Answer__: PROVIDE YOUR ANSEWR HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Answer 9**\n",
        "\n",
        "What is happening here is that one single linear model is trying to fit data that actually comes from two different groups. The dataset is not one clean straight line pattern, so one global line does a bad job. That is why the full dataset result looks terrible, while each subset gives much smaller error.\n",
        "\n",
        "There is also a mismatch in the code that makes the numbers look even worse. The model is trained to predict MFR, but the R^2 and mse are computed against lnMFR. Since those are different scales and different targets, the score can become very negative and misleading. So the huge change in R^2 and mse is caused by both data structure and evaluation inconsistency.\n",
        "\n",
        "So no, it was not a good idea to blindly use one linear regression on the whole dataset without checking the distribution first. A better approach is to either model the two groups separately or use a model that can handle nonlinear or multi-regime behavior."
      ],
      "metadata": {
        "id": "2V08gzMsjEfZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gczfjKF9S_0H"
      },
      "source": [
        "## **Question 10** **($\\infty$ points, since this is where you really learn stuff! :-)**\n",
        "\n",
        "You can get as many points of extra credit as you like by doing a good job with this question!\n",
        "\n",
        "Play around with doing a better analysis.  Things to try:\n",
        "\n",
        "1.   Redo the calculations, but start from the split dataset above.\n",
        "2.   Try different train and test splits and see if the answer changes (i.e.,\n",
        "```\n",
        "train_processed,test_processed = train_test_split(ds, test_size=0.2, shuffle=True, random_state=1234)\n",
        "```\n",
        "3.   Try different methods such as `linear_model.LinearRegression()`, `KNeighborsRegressor(n_neighbors=20)`, `linear_model.Ridge()`, `linear_model.Lasso()`, or `linear_model.BayesianRidge()`. Can you achieve better performance than any of the previous models?\n",
        "\n",
        "4.  Answer the following question carefully:  What is the problem with using this method of selecting a model based on its performance on testing data?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Answer 10**\n",
        "\n",
        "We trained the models on 1617 samples and evaluated on 412 test samples. Based on 5 fold cross validation, KNN with k equals 7 was clearly the best model. It had the highest average R^2 at about 0.786 and the lowest average MSE around 4.88. The other models were noticeably weaker, with R^2 values around 0.63 for linear methods and about 0.67 for the decision tree. After selecting KNN from cross validation and testing it once on the untouched test set, it still performed well with test R^2 of 0.7288 and test MSE of 6.0910. We also repeated cross validation with different random splits, and the scores changed a little each time, which is expected. The KNN average R^2 stayed in a narrow range around 0.776 to 0.788 and MSE stayed around 4.85 to 5.10, so the model looks fairly stable. This shows why trying different splits matters, because one split alone can be misleading, while repeated cross validation gives a more reliable picture of performance."
      ],
      "metadata": {
        "id": "sqPh_40mlZHr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lctVJxlxlYRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "MjqBnd-N5b4y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61403889-9bd7-47db-e6ac-de06798722a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train samples: 1617\n",
            "Test samples : 412\n",
            "cross-validation results (train only, seed=42):\n",
            "                    Model  CV_R2_mean  CV_R2_std  CV_MSE_mean  CV_MSE_std\n",
            "                 KNN(k=7)    0.785712   0.048311     4.883443    1.125820\n",
            "DecisionTree(max_depth=6)    0.672073   0.074711     7.446001    1.612667\n",
            "            BayesianRidge    0.627462   0.037944     8.476647    0.868042\n",
            "         Ridge(alpha=1.0)    0.627462   0.037946     8.476637    0.867891\n",
            "         LinearRegression    0.627459   0.037947     8.476707    0.867863\n",
            "Best model selected from CV: KNN(k=7)\n",
            "Final TEST R^2: 0.7288\n",
            "Final TEST MSE: 6.0910\n",
            "Effect of different splits for best model: KNN(k=7)\n",
            " Seed  CV_R2_mean  CV_R2_std  CV_MSE_mean  CV_MSE_std\n",
            "    0    0.776127   0.034687     5.097120    0.919184\n",
            "    7    0.779715   0.016427     5.005444    0.403340\n",
            "   21    0.786103   0.035835     4.890694    0.913558\n",
            "   42    0.785712   0.048311     4.883443    1.125820\n",
            "   99    0.787855   0.028583     4.851218    0.823856\n",
            "  123    0.785169   0.028514     4.880742    0.728012\n",
            " 2026    0.780170   0.061556     4.986089    1.365767\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import KFold, cross_validate\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, Ridge, BayesianRidge\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "FEATURES = ['C3', 'H2R', 'Pressure', 'Level', 'C2', 'Cat', 'Temp']\n",
        "TARGET = 'MFR'\n",
        "\n",
        "train_q10 = train.dropna(subset=FEATURES + [TARGET]).copy()\n",
        "test_q10  = test_dropna.dropna(subset=FEATURES + [TARGET]).copy()\n",
        "\n",
        "X_train = train_q10[FEATURES].to_numpy()\n",
        "y_train = train_q10[TARGET].to_numpy()\n",
        "\n",
        "X_test = test_q10[FEATURES].to_numpy()\n",
        "y_test = test_q10[TARGET].to_numpy()\n",
        "\n",
        "print(\"Train samples:\", len(train_q10))\n",
        "print(\"Test samples :\", len(test_q10))\n",
        "\n",
        "models = {\n",
        "    \"LinearRegression\": Pipeline([(\"scaler\", StandardScaler()),(\"model\", LinearRegression())]),\n",
        "    \"Ridge(alpha=1.0)\": Pipeline([(\"scaler\", StandardScaler()),(\"model\", Ridge(alpha=1.0, random_state=42))]),\n",
        "    \"BayesianRidge\": Pipeline([(\"scaler\", StandardScaler()),(\"model\", BayesianRidge())]),\n",
        "    \"KNN(k=7)\": Pipeline([(\"scaler\", StandardScaler()),(\"model\", KNeighborsRegressor(n_neighbors=7))]),\n",
        "    \"DecisionTree(max_depth=6)\": Pipeline([(\"model\", DecisionTreeRegressor(max_depth=6, random_state=42))]),\n",
        "}\n",
        "\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scoring = {\"r2\": \"r2\", \"neg_mse\": \"neg_mean_squared_error\"}\n",
        "\n",
        "rows = []\n",
        "for name, pipe in models.items():\n",
        "    cv_out = cross_validate(pipe,X_train, y_train,cv=cv,scoring=scoring,return_train_score=False,n_jobs=-1)\n",
        "\n",
        "    mean_r2  = np.mean(cv_out[\"test_r2\"])\n",
        "    std_r2   = np.std(cv_out[\"test_r2\"])\n",
        "    mean_mse = -np.mean(cv_out[\"test_neg_mse\"])\n",
        "    std_mse  = np.std(-cv_out[\"test_neg_mse\"])\n",
        "\n",
        "    rows.append({\n",
        "        \"Model\": name,\n",
        "        \"CV_R2_mean\": mean_r2,\n",
        "        \"CV_R2_std\": std_r2,\n",
        "        \"CV_MSE_mean\": mean_mse,\n",
        "        \"CV_MSE_std\": std_mse\n",
        "    })\n",
        "\n",
        "cv_results = pd.DataFrame(rows).sort_values(by=\"CV_R2_mean\", ascending=False)\n",
        "\n",
        "print(\"cross-validation results (train only, seed=42):\")\n",
        "print(cv_results.to_string(index=False))\n",
        "\n",
        "\n",
        "best_model_name = cv_results.iloc[0][\"Model\"]\n",
        "best_pipe = models[best_model_name]\n",
        "best_pipe.fit(X_train, y_train)\n",
        "\n",
        "y_pred_test = best_pipe.predict(X_test)\n",
        "test_r2 = r2_score(y_test, y_pred_test)\n",
        "test_mse = mean_squared_error(y_test, y_pred_test)\n",
        "\n",
        "print(\"Best model selected from CV:\", best_model_name)\n",
        "print(f\"Final TEST R^2: {test_r2:.4f}\")\n",
        "print(f\"Final TEST MSE: {test_mse:.4f}\")\n",
        "\n",
        "\n",
        "seeds = [0, 7, 21, 42, 99, 123, 2026]\n",
        "split_rows = []\n",
        "\n",
        "for seed in seeds:\n",
        "    cv_seed = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "    cv_out_seed = cross_validate(best_pipe,X_train, y_train,cv=cv_seed,scoring=scoring,return_train_score=False,n_jobs=-1)\n",
        "\n",
        "    split_rows.append({\n",
        "        \"Seed\": seed,\n",
        "        \"CV_R2_mean\": np.mean(cv_out_seed[\"test_r2\"]),\n",
        "        \"CV_R2_std\": np.std(cv_out_seed[\"test_r2\"]),\n",
        "        \"CV_MSE_mean\": -np.mean(cv_out_seed[\"test_neg_mse\"]),\n",
        "        \"CV_MSE_std\": np.std(-cv_out_seed[\"test_neg_mse\"])\n",
        "    })\n",
        "\n",
        "split_results = pd.DataFrame(split_rows).sort_values(by=\"Seed\")\n",
        "\n",
        "print(f\"Effect of different splits for best model: {best_model_name}\")\n",
        "print(split_results.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p7YlY1WZkbkR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}